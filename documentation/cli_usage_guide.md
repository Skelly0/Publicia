# Publicia CLI (`cli.py`) Usage Guide (for LLMs/Automation)

This guide explains how to use the `cli.py` script to interact with the Publicia knowledge base from a command-line environment.

## Purpose

The `cli.py` script provides a direct interface to Publicia's core query functionality. It allows submitting a question (and optionally an image URL) and receiving a text-based answer generated by an LLM, informed by relevant documents from the knowledge base.

## Prerequisites

1.  **Python Environment:** A Python 3 environment (matching the project's requirements) must be available.
2.  **Dependencies:** All packages listed in `requirements.txt` must be installed in the environment where `cli.py` is executed. This can typically be done using `pip install -r requirements.txt`.
3.  **Configuration:** A valid `.env` file must be present in the project root directory, containing necessary API keys (`OPENROUTER_API_KEY`, `GOOGLE_API_KEY`) and potentially default model settings.
4.  **Data:** The `documents/processed_data` and `images` directories must contain the necessary pre-processed document embeddings, metadata, and image files used by the `DocumentManager` and `ImageManager`. The script initializes these managers and expects the data to be present.

## Command Structure

The script is invoked using the Python interpreter:

```bash
python cli.py [options] "Your question here"
```

## Arguments

1.  **`question`** (Positional, Required)
    *   Type: `string`
    *   Description: The natural language question to ask the Publicia knowledge base. Must be enclosed in quotes if it contains spaces.

2.  **`--image-url`** (Optional)
    *   Type: `string`
    *   Description: A direct URL to an image file (e.g., ending in `.jpg`, `.png`). If provided, the script will download the image, encode it, and pass it to a vision-capable LLM along with the question and text context.

3.  **`--model`** (Optional)
    *   Type: `string`
    *   Description: Specifies the exact OpenRouter model identifier (e.g., `anthropic/claude-3.5-sonnet`, `google/gemini-2.0-flash-001`) to use for generating the response. If omitted, the script uses the `LLM_MODEL` defined in the `.env` file or the hardcoded default in `managers/config.py`.

## Execution Flow

1.  **Initialization:**
    *   Parses command-line arguments.
    *   Configures logging (output typically goes to stderr).
    *   Initializes `Config`, `DocumentManager`, `ImageManager`, and `UserPreferencesManager`.
    *   Loads document data (embeddings, metadata) and image metadata asynchronously. Errors during loading may prevent successful searching.
2.  **Image Processing (if `--image-url` is provided):**
    *   Downloads the image from the URL.
    *   Encodes the image to base64.
3.  **Document Search:**
    *   Generates an embedding for the question.
    *   Performs a hybrid search (embedding + BM25) using `DocumentManager.search`.
    *   Applies re-ranking if enabled in the configuration.
    *   Logs the number of relevant document sections found.
4.  **Context Formatting:**
    *   Formats the retrieved document chunks (and image descriptions, if any) into a context string for the LLM.
    *   Adds citation information (e.g., Google Doc URLs) where available.
5.  **LLM Interaction (`_cli_try_ai_completion`):**
    *   Constructs the message list (system prompt, document context, image data if applicable, user question).
    *   Attempts to get a completion from the specified or default model using the OpenRouter API.
    *   **Fallback Logic:** If the initial model fails (e.g., rate limit 429, API error, timeout), it automatically attempts a sequence of predefined fallback models (currently: `anthropic/claude-3.5-sonnet`, `google/gemini-2.0-flash-001`, `mistralai/mistral-large`).
    *   Uses the `API_TIMEOUT` value from the `Config` object for the API call timeout.
6.  **Output:**
    *   If a valid response is received from any model, logs the model used and prints the formatted response content to standard output (stdout).
    *   If all models fail or an error occurs, prints an error message to standard error (stderr) and logs details.

## Output Format

*   **Logs:** Informational, warning, and error messages are printed to standard error (stderr).
*   **Response:** The final AI-generated answer is printed to standard output (stdout), typically preceded and followed by separator lines (`--- Publicia Response ---` and `-----------------------`).
*   **Errors:** Critical errors during execution (e.g., manager initialization failure, unhandled exceptions) may cause the script to exit with a non-zero status code and print error messages to stderr.

## Example Invocations

```bash
# Basic query using default model
python cli.py "Describe the history of House Aleph."

# Query with an image URL, using default model
python cli.py "What is shown in this image related to Ledus Banum?" --image-url "https://example.com/ledus_image.png"

# Query specifying a particular model
python cli.py "Explain the role of the Universal Temple." --model "qwen/qwq-32b"

# Query where the default model might be rate-limited (will trigger fallback)
python cli.py "What are the main regions of Ledus Banum 77?"
```

## Notes for LLMs

*   The script relies heavily on `asyncio` for asynchronous operations (manager loading, API calls).
*   The core logic involves RAG (Retrieval-Augmented Generation): searching documents (`DocumentManager`), formatting context, and then calling an LLM (`_cli_try_ai_completion`).
*   Error handling is present, but specific API errors or data loading issues might require inspection of the logs (stderr) for diagnosis.
*   The fallback model list is hardcoded within `_cli_try_ai_completion` in `cli.py`.
